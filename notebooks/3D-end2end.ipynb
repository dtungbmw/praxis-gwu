{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db7415d-076f-4448-9117-e152c53ecc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://youtu.be/5ypQIUbpA7c?si=OjF6L76MTJJgSrBs\n",
    "# works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57202d5-c15b-4cb3-94fa-c6d584ba2bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 19:01:57.443104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from transformers import GLPNImageProcessor, GLPNForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecb6657-1ac9-4cfa-adf0-89af98f31947",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"images/pointing.jpg\"\n",
    "video_file = \"Videos/pointing.mp4\"\n",
    "video_file = \"Videos/pointing.MOV\"\n",
    "image_file = \"images/tmp2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e6bfd5-170c-4929-adea-37a26f766f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img(video_file):\n",
    "    cam = cv2.VideoCapture(video_file)\n",
    "    for i in range(0,40):\n",
    "        ret,frame = cam.read() # read 1st image\n",
    "    cv2.imwrite(\"./images/tmp2.jpg\", frame)\n",
    "    cam.release()\n",
    "    return\n",
    "\n",
    "extract_img(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df767aa-1709-4724-a012-070966c9e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = GLPNImageProcessor.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-nyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b37f56a-067c-46d7-b460-fb1c83e38653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/davidtung/Documents/GitHub/GWU-pointing/notebooks/images/tmp2.jpg: 384x640 1 lamp, 164.9ms\n",
      "Speed: 2.6ms preprocess, 164.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "box-> \n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8806])\n",
      "data: tensor([[290.1316, 201.0190, 399.8622, 393.7111,   0.8806,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[344.9969, 297.3651, 109.7307, 192.6921]])\n",
      "xywhn: tensor([[0.2695, 0.4130, 0.0857, 0.2676]])\n",
      "xyxy: tensor([[290.1316, 201.0190, 399.8622, 393.7111]])\n",
      "xyxyn: tensor([[0.2267, 0.2792, 0.3124, 0.5468]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# detection\n",
    "from ultralytics import YOLOWorld\n",
    "\n",
    "def obj_detect(image):\n",
    "    # Initialize a YOLO-World model\n",
    "    model = YOLOWorld(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt for different sizes\n",
    "    model.set_classes([\"laptop\" , \"lamp\", \"fan\"])\n",
    "    results = model.predict(image)\n",
    "\n",
    "    for result in results:\n",
    "        print(\"box-> \")\n",
    "        print(result.boxes)\n",
    "        print(result.probs)\n",
    "\n",
    "    # Show results\n",
    "    results[0].show()\n",
    "    return results[0]\n",
    "bounding=obj_detect(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7275d758-376d-404c-b581-49b284c33fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_file)#.convert(\"RGB\")\n",
    "new_height = 480 if image.height > 480 else image.height\n",
    "new_height -= (new_height %32)\n",
    "new_width = int(new_height * image.width / image.height)\n",
    "diff = new_height % 32\n",
    "\n",
    "new_width = new_width - diff if diff < 16 else new_width +32 - diff\n",
    "new_size = (new_width,new_height)\n",
    "image = image.resize (new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce961b59-f432-425d-a231-8d562be4cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model (**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d5dcf0-cc75-4b8f-a20f-80eb20267822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "pad = 16\n",
    "#pad = (image.width - predicted_depth.shape[0]) // 2  # p for padding\n",
    "print(pad)\n",
    "output = predicted_depth.squeeze().cpu().numpy() * 1000.0\n",
    "output = output[pad:-pad, pad:-pad]\n",
    "image = image.crop((pad, pad, image.width - pad, image.height - pad ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b7e30b-4bd5-48d6-ac97-e190604ac7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   -0.97031     0.97031     -1.9406]\n",
      " [   -0.97027      0.9727     -1.9454]\n",
      " [   -0.97089     0.97577     -1.9515]\n",
      " ...\n",
      " [    0.90149    -0.90424     -1.8166]\n",
      " [    0.90326    -0.90375     -1.8156]\n",
      " [    0.90616    -0.90437     -1.8169]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def depth_to_point_cloud(depth_map, scale_factor=0.001):\n",
    "    h, w = depth_map.shape\n",
    "    i, j = np.meshgrid(np.arange(w), np.arange(h), indexing='xy')\n",
    "    z = depth_map * scale_factor\n",
    "    x = (i - w / 2) * z / w\n",
    "    y = (j - h / 2) * z / h\n",
    "    z = -z  # Invert the y-axis\n",
    "    y = -y  # Invert the y-axis\n",
    "    points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n",
    "    return points\n",
    "\n",
    "point_cloud = depth_to_point_cloud(output)\n",
    "print(point_cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b96e356-f037-4144-93ff-4cae4e5f8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_point_cloud(points):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=0.1)  # Plot the points\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4fe92d4-282c-428d-b624-349475e8703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(image)\n",
    "#ax[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "ax[1].imshow(output, cmap='plasma')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.pause(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6adb472d-89c5-45af-bba7-7ea67a2f41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ec57d2-4c8b-4dc3-932b-a358e2a93aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd9e7e0-c379-492a-b95b-c91f5d1cb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation matrices\n",
    "rotation_y_180 = np.array([\n",
    "    [-1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "rotation_z_180 = np.array([\n",
    "    [-1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae0a1e5-c4f0-41c4-a623-81d4cb9b6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation2(points, transformation_matrix):\n",
    "    ones = np.ones((points.shape[0], 1))\n",
    "    points_homogeneous = np.hstack((points, ones))  # Convert to homogeneous coordinates\n",
    "    transformed_points_homogeneous = points_homogeneous @ transformation_matrix.T\n",
    "    transformed_points = transformed_points_homogeneous[:, :3]  # Convert back to 3D coordinates\n",
    "    return transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d915602-d63e-495e-ad63-b94cf37abfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(point_cloud, transformation_matrix):\n",
    "    point_cloud.transform(transformation_matrix)\n",
    "    return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c649715-ae0a-417c-b09e-fc100965f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_3d_coordinates(point_cloud):\n",
    "    points = np.asarray(point_cloud.points)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955df051-3f6c-4827-86f8-0a59e162d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.size\n",
    "depth_image = (output * 255 / np.max(output)).astype('uint8')\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b75e517-a17a-410a-b4e7-7cbd3293c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_o3d_image(o3d_image):\n",
    "    np_image = np.asarray(o3d_image)\n",
    "\n",
    "    # Display the image using Matplotlib\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f25db72-0cec-4ac7-b45a-41ee9dc9a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448, 821)\n",
      "(448, 821, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "color_height, color_width = image.shape[:2]\n",
    "depth_image = cv2.resize(depth_image, (color_width, color_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "depth_03d = o3d.geometry.Image(depth_image)\n",
    "image_03d = o3d.geometry.Image(image)\n",
    "print(depth_image.shape)\n",
    "print(image.shape)\n",
    "#show_o3d_image(depth_image)\n",
    "#show_o3d_image(image)\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth \\\n",
    "(image_03d,depth_03d, depth_scale=1000.0, depth_trunc=3.0,\\\n",
    " convert_rgb_to_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2017a62-474e-4d46-a9e0-3a57609bea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera_intrinsic.set_intrinsics(width, height, 500, 500, width/2, height/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc4bb91-141e-4997-8f1b-eebc3b098162",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, camera_intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918b1817-5a9a-45b0-9536-e4c6258a6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "pcd = apply_transformation(pcd, rotation_y_180)\n",
    "pcd = apply_transformation(pcd, rotation_z_180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d678417-95a1-4f75-a81b-c06298dc85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(np.asarray((pcd.points)))\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afabdb4-c248-4d24-9ea4-51e67bdfe44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = extract_3d_coordinates(pcd)\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867789c-0164-429d-a6c1-03dba076e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123931a0-c4e0-46d0-99c4-1104c5a426fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
